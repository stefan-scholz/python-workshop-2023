{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "elder-blink",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python Workshop\n",
    "# Session 2: Working with Structured Data\n",
    "\n",
    "Stefan Scholz\n",
    "\n",
    "In this second session we will build an exemplary **data pipeline** with Python. For this, we will work with **external packages** to **import** and **clean data**, then perform **simple analyses** and finally make **visualizations**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781bf774",
   "metadata": {},
   "source": [
    "## 2.1 Data Sources\n",
    "\n",
    "In practice, there are many different data sources available where you can get data from. Each source has its own advantages and disadvantages. So far we have worked a lot with variables in our code, but they are not suitable for storing big amounts of data. That is why we also used files which can store more data, but they are difficult to exchange. This is where application programming interface (API) have emerged that provide an interface for exchanging data between clients and servers. At the same time, we will look at how to get data from websites. Behind APIs and websites, there is usually a database system in which all data is systematically stored and made available. These databases can also be used directly from Python.\n",
    "\n",
    "Below is a short (and probably incomplete) list of data sources.\n",
    "\n",
    "| Source | Description |\n",
    "| -------- | ------- |\n",
    "| Variable | reserved memory location to store data |\n",
    "| File | physical storage to store data |\n",
    "| Website | online available web resources |\n",
    "| API | interface for exchanging data between clients and servers |\n",
    "| Database | organized storage and access with software |\n",
    "\n",
    "\n",
    "We will begin to work with files from the [tree cadastre of the city of Konstanz](https://offenedaten-konstanz.de/dataset/baumkataster-konstanz)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b864d2ac",
   "metadata": {},
   "source": [
    "## 2.2 Data Handling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9241689e",
   "metadata": {},
   "source": [
    "### Manual\n",
    "\n",
    "To read and write data to files, the module `csv` provides a lot of functionalities. This way you can get data into and out of your program. In CSV files you can find tabular data as comma-separated values. Each line is a data record. And each record has one or more fields which are comma separated.\n",
    "\n",
    "Let us first import the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fe4a43",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8c1a7f",
   "metadata": {},
   "source": [
    "Suppose we want to work with the files from the tree cadastre of the city of Konstanz. You can load the files by opening the file in read mode `r` and a reader is created like `csv.reader()`. Then you can get the rows of the CSV file as a sequence from the reader, e.g. with a `for` loop.\n",
    "\n",
    "The tree cadastre data has already been saved in this repository. Alternatively you can also download it from the [open data portal of the city of Konstanz](https://offenedaten-konstanz.de/dataset/baumkataster-konstanz).\n",
    "\n",
    "Let us try to read the tree cadastre from the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16023eb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# open file\n",
    "with open(file=\"data/KN_Baumkataster_2020.csv\", mode=\"r\") as csv_file:\n",
    "    # create reader\n",
    "    csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
    "\n",
    "    # iterate over rows in file\n",
    "    for row in csv_reader:\n",
    "        # print row\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96dcb49",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0139122",
   "metadata": {},
   "source": [
    "Another file format is JSON, which is a standardized and common data format to store and interchange data independent from any programming language. JSON data types are numbers, Unicode strings, boolean values, the `null` value (`None`), arrays (Python lists) and objects (Python dictionaries). The JSON data types and the JSON syntax are similar to Python. But there are subtle differences and we use the module `json` to read or write JSON data:\n",
    "\n",
    "Let us first import the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e05476",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170623a",
   "metadata": {},
   "source": [
    "We have prepared translations of the tree names in the tree cadastre. These translations were obtained from the [Wikispecies project](https://species.wikimedia.org/wiki/Main_Page) via the [Mediawiki API](https://www.mediawiki.org/wiki/API:Main_page). We will later learn how to use an [API](https://en.wikipedia.org/wiki/API) (Application Programming Interface) later.\n",
    "\n",
    "Let us read the translations from the JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1338a6de",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# open file\n",
    "with open(file=\"data/trees-wikispecies.json\", mode=\"r\") as json_file:\n",
    "    # read json data\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "# print translations for one tree\n",
    "json_data[\"Gleditsia triacanthos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9788b638",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ccfdfb8",
   "metadata": {},
   "source": [
    "### Pandas\n",
    "\n",
    "Now you have seen how you can manually load the data from files. Maybe you have realized that this exercise is very time-consuming because many little steps must be implemented: How do I open the file? How do I iterate over the lines? How do I save it?\n",
    "\n",
    "That is why developers have written external packages for data handling. For example, in these packages files can be loaded in one line of code. One of these packages is `pandas` and it supports all kinds of different data sources.\n",
    "\n",
    "The package `pandas` helps you to arrange your data like tables. Through `pandas`, you can import, clean, transform, analyse and export data.\n",
    "\n",
    "Let us first import the package or install it again if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb12ddb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2559cb",
   "metadata": {},
   "source": [
    "The primary component of `pandas` is its `DataFrame`. A `DateFrame` is organized like a table and has rows and columns. There are several ways to create a new `DataFrame`, but the easiest way to start is to take a dictionary and pass it into a new `DataFrame`.\n",
    "\n",
    "When you want to load data from files, you can load it as `DataFrame` with `pandas` from a CSV or JSON files with the functions `pd.read_csv()` and `pd.read_json()`.\n",
    "\n",
    "Let us load the tree cadastre with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db5d648",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# read dataframe\n",
    "trees = pd.read_csv(\"data/KN_Baumkataster_2020.csv\")\n",
    "\n",
    "# print dataframe\n",
    "print(trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4582e40",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbfa42f2",
   "metadata": {},
   "source": [
    "As your datasets become larger and non-trivial, you will not be able to print the entire data set anymore. `pandas` offers two methods `head()` and `tail()` to display either the first or the last rows of a `DataFrame`. By default, 10 rows are shown, but you can select any other number too.\n",
    "\n",
    "Let us print the first and last row once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52c6f7f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# print first row\n",
    "print(trees.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ced61b4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# print last row\n",
    "print(trees.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bf2819",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e5ae9d2",
   "metadata": {},
   "source": [
    "Instead of accessing the entire `DateFrame`, you can also restrict the selection to certain columns, rows and cells. Columns can be easily restricted with an index with a list of their names. Rows can either be selected by their name using the method `loc[]` or by their index using the method `iloc[]`. Of course you can also restrict both columns and rows at the same time.\n",
    "\n",
    "Let us select different parts of our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dde1ed",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# select column of Name_dt\n",
    "trees_selected = trees.loc[:, [\"Name_dt\"]]\n",
    "\n",
    "# print selected dataframe\n",
    "print(trees_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f10292e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# select rows with tree baumId 39080\n",
    "trees_selected = trees[trees[\"baumId\"] == 1970]\n",
    "\n",
    "# print selected dataframe\n",
    "print(trees_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba38bf6e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b1196bc",
   "metadata": {},
   "source": [
    "When you want to select parts of a `DataFrame` which fulfill a certain condition, then you can also write a conditional statement instead of a name or index. You then write the statement again as a kind of index behind the `DataFrame`. Then you get all parts of the `DateFrame` for which the condition is correct. Of course you can also combine several conditions in round brackets `(` `)` with logical ands `&` and logical ors `|`.\n",
    "\n",
    "Let us select trees according to their prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b6b7a6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# select rows with tree name Populus nigra 'Italica' and higher than 20 meters\n",
    "trees_selected = trees[(trees[\"Name_lat\"] == \"Populus nigra 'Italica'\") & (trees[\"hoeheM\"] > 30)]\n",
    "\n",
    "# print selected dataframe\n",
    "print(trees_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b9a340",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b20955df",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise</b>: Inspect the dataframe. Select the trees with a height smaller than 20 meters and a trunk perimeter of more than 600 centimeters.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad562be",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59782ec",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e1126d8",
   "metadata": {},
   "source": [
    "There are also several ways to change a `DataFrame` in `pandas`. A few ways are for example to append a `DataFrame` to another `DataFrame` with `append()`, find and delete duplicate rows with `drop_duplicates()`, sort rows with `sort_values()` and find rows with missing values and delete them with `dropna()`. Note that certain operations only pass a reference and others pass a new `DataFrame`. If you want to write to the exact same `DataFrame`, you can in most cases pass the argument `inplace = True` to the function.\n",
    "\n",
    "Let us clean up the data a little bit by translating the German column names, dropping the columns not used later on and using the column \"OBJECTID\" as row index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58725975",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# define columns rename\n",
    "columns_rename = {\"hoeheM\": \"height (m)\", \"kronendurchmesserM\": \"treetop diameter (m)\", \"stammumfangCM\": \"trunk perimeter (cm)\"}\n",
    "\n",
    "# define columns drop\n",
    "columns_drop = [\"baumId\", \"baumNr\", \"baumart\", \"AGOL_Name\"]\n",
    "\n",
    "# rename columns in dataframe\n",
    "trees.rename(columns=columns_rename, inplace=True)\n",
    "\n",
    "# drop columns in dataframe\n",
    "trees.drop(columns=columns_drop, inplace=True)\n",
    "\n",
    "# set index in dataframe\n",
    "trees.set_index(\"OBJECTID\", inplace=True)\n",
    "\n",
    "# print head of dataframe\n",
    "trees.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a25c87",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9984d728",
   "metadata": {},
   "source": [
    "## 2.3 Data Analysis\n",
    "\n",
    "On the basis of the completed data handling, the next step in our data pipeline is to start with the data analysis. Descriptive statistics help us in the first place to better understand the data, e.g. means, quantiles, deviations, counts etc. These are implemented in `pandas`. To model the data and recognize mechanisms we move on to inferential statistics, e.g. correlations, regressions etc. For these statistics we will introduce `sklearn`. But let us take a look at all packages one after the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cabd59e",
   "metadata": {},
   "source": [
    "### Pandas\n",
    "\n",
    "Within a `DateFrame` of `pandas`, you can use the method `describe()` to view the basic statistical characteristics of each feature. These characteristics can also be calculated individually using e.g. the methods `max()`, `min()` and `mean()`. Or additionally calculate the methods `sum()` and `corr()`, which should explain themselves by their names.\n",
    "\n",
    "Let us have a look on the basic statistical characteristics of our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59cf6b6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# print characteristics\n",
    "print(trees.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33293715",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# define columns numeric\n",
    "columns_numeric = [\"trunk perimeter (cm)\", \"height (m)\", \"treetop diameter (m)\"]\n",
    "\n",
    "# print correlations\n",
    "print(trees.loc[:, columns_numeric].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dcd5cb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# count tree names and show the N most frequent tree names\n",
    "trees_top = trees[\"Name_lat\"].value_counts().head(20).to_frame()\n",
    "\n",
    "# print top trees\n",
    "print(trees_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2486b69",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8320b1e4",
   "metadata": {},
   "source": [
    "## Sklearn\n",
    "\n",
    "A new package, we have not seen yet, is `sklearn` or `scikit-learn`. `sklearn` offers different machine learning methods like linear models, support vector machines, tree-based methods, nearest neighbors, neural networks, clustering, matrix decomposition... Each of these methods can be conveniently prepared, performed and evaluated. To demonstrate how easy an analysis in `sklearn` is, we will run a linear regression. First, we will demonstrate it on our trivial data set, but then you will do it by yourself on your data set.\n",
    "\n",
    "Let us first import from the package its linear model or install it again if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88721c2e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03147aca",
   "metadata": {},
   "source": [
    "The first step is to prepare the underlying data as `numpy` array, where the dependent variable is usually stored in `Y`, and the independent variables are stored in `X`. The next step is to create a linear regression model using `sklearn`. This model can be computed by passing the dependent and independent variables with the method `fit()`. To get some results of the model, you can retrieve the intercept from `intercept_`, the coefficients from `coef_`, and all kinds of errors.\n",
    "\n",
    "Let us run our first linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d97bbe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# keep trees without nans\n",
    "trees_notna = trees.dropna(subset=[\"height (m)\", \"trunk perimeter (cm)\"])\n",
    "\n",
    "# prepare dependent variable\n",
    "Y = trees_notna[\"height (m)\"].to_numpy()\n",
    "\n",
    "# prepare independent variable\n",
    "X = trees_notna[\"trunk perimeter (cm)\"].to_numpy().reshape(-1, 1)\n",
    "\n",
    "# create linear regression model\n",
    "regression = linear_model.LinearRegression()\n",
    "\n",
    "# compute linear regression model\n",
    "regression.fit(X, Y)\n",
    "\n",
    "# print intercept\n",
    "print(\"Intercept is\", regression.intercept_)\n",
    "\n",
    "# print coefficients\n",
    "print(\"Coefficients are\", regression.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c96143a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01e5fdbc",
   "metadata": {},
   "source": [
    "## 2.3 Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c8d485",
   "metadata": {},
   "source": [
    "### Matplotlib\n",
    "\n",
    "The package `matplotlib` is de facto the visualization tool in Python. It allows us to create all possible kinds of visualizations and get further insights into our data. You can basically use `matplotlib` out of `pandas`, but we will use it as a standalone package for the sake of clarity.\n",
    "\n",
    "A figure in `matplotlib` consists in principle of the following components:\n",
    "\n",
    "| Component | Description |\n",
    "| -------- | ------- |\n",
    "| Figure | canvas which contains one or more axes |\n",
    "| Axes | plot with one axis per dimension |\n",
    "| Axis | number line like object |\n",
    "\n",
    "Let us first make the necessary imports and choose the [matplotlib's style \"ggplot\"](https://matplotlib.org/stable/gallery/style_sheets/ggplot.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b2865e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cfb391",
   "metadata": {},
   "source": [
    "For our first plot, we create a first trivial scatter plot of the 3 metric values. For that, we call on the dataframe `trees` the method `plot()`, which creates the actual plot. With the method `show()` we invoke all previously created plots.\n",
    "\n",
    "Let us create our first plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f9eef6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# create plot\n",
    "trees.plot(kind=\"scatter\", x=\"trunk perimeter (cm)\", y=\"height (m)\", s=\"treetop diameter (m)\")\n",
    "\n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2602abdf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bba8ddc",
   "metadata": {},
   "source": [
    "For our next plot, we will take into account the tree types. We will focus on the top-20 most frequent names only and plot the metrics per tree on a 4x5 matrix and add some color to the plots.\n",
    "\n",
    "Let us create our next plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea8b544",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# create plot as grid of subplots\n",
    "fig, axes = plt.subplots(nrows=5, ncols=4, sharex=True, sharey=True, squeeze=False, figsize=[20,25])\n",
    "\n",
    "# iterate over top-20 most frequent names and subplots\n",
    "n = 0\n",
    "for tree in trees_top.index.to_list():\n",
    "    trees[trees[\"Name_lat\"]==tree].plot(\n",
    "        kind=\"scatter\",\n",
    "        ax=axes[int(n/4),n%4],\n",
    "        title=tree,\n",
    "        x=\"trunk perimeter (cm)\",\n",
    "        y=\"height (m)\",\n",
    "        s=\"treetop diameter (m)\", # show by point size\n",
    "        c=\"treetop diameter (m)\", # also indicated by color\n",
    "        colormap=\"Spectral\",\n",
    "        norm=matplotlib.colors.LogNorm(vmin=1, vmax=25),\n",
    "        colorbar=None)\n",
    "    n += 1\n",
    "\n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5363573",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c133b959",
   "metadata": {},
   "source": [
    "### Folium\n",
    "\n",
    "The package `folium` allows us to plot geographic data, as the ones from the trees in the tree cadastre of the city Konstanz. In this dataset, every tree has a longitude and latitude. The locations of the trees can be visualized in an interactive map using `folium`.\n",
    "\n",
    "Let us import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b4dea",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "import branca.colormap as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4ca9f3",
   "metadata": {},
   "source": [
    "To create an interactive map, we create a map `folium.Map`. In this map, each tree will have a marker where the color of the marker depends on the height and the radius of the marker on the diameter.\n",
    "\n",
    "Let us create this interactive map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e87f967",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# create map\n",
    "map = folium.Map(location=[47.66336, 9.17598], tiles=\"Stamen Terrain\", zoom_start=16)\n",
    "\n",
    "# create colormap\n",
    "colormap = cm.LinearColormap(colors=[\"lightgreen\", \"darkgreen\"], vmin=1, vmax=40).to_step(n=12)\n",
    "\n",
    "# define helper function for color\n",
    "def color_height(height):\n",
    "    if 1.0 <= height <= 40.0:\n",
    "        return colormap(height)\n",
    "    else:\n",
    "        return \"darkblue\"\n",
    "\n",
    "# define helper function to add trees to map\n",
    "def map_tree(row):\n",
    "    marker = folium.CircleMarker(\n",
    "        location=(row[\"Y\"], row[\"X\"]),\n",
    "        tooltip=folium.Tooltip(row[\"Name_lat\"]),\n",
    "        radius=row[\"treetop diameter (m)\"]/4,\n",
    "        fill=True,\n",
    "        color=color_height(row[\"height (m)\"]),\n",
    "    )\n",
    "    marker.add_to(map)\n",
    "\n",
    "# add trees to map\n",
    "trees.apply(map_tree, axis=1)\n",
    "\n",
    "# modify map by color height\n",
    "map.add_child(colormap, name=\"height (m)\")\n",
    "\n",
    "# show map\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de449691",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
