{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "stupid-remedy",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python Workshop\n",
    "# Session 4: Web Scraping\n",
    "\n",
    "Stefan Scholz\n",
    "\n",
    "In this fourth session we will learn how to crawl websites. By doing this, we will get familiar with HTTP requests, data extraction and browser automation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55555c16",
   "metadata": {},
   "source": [
    "## 4.1 Websites\n",
    "\n",
    "Another web resource in the world wide web is webpages. Every day we use hundreds of websites like [Google](https://www.google.com/), [Wikipedia](https://frr.wikipedia.org/wiki/), [Youtube](https://www.youtube.com/), [Facebook](https://www.facebook.com/), [StackOverflow](https://stackoverflow.com/), and [GitHub](https://github.com/) to get information. These websites usually consist of several webpages written in HTML or a comparable markup language. So why should we not also be able to stream data from webpages?\n",
    "\n",
    "Yes, in general we are able to stream our data also from webpages with Python. But in comparision to REST APIs it requires much more effort, time and tears, because the desired information has not been collected, cleaned or structured by a provider. Still, if there is no suitable API available or it is unreasonable expensive, it is a good idea to implement a so-called web scraper. They are used to extract data from webpages automatically either with a bot or web crawler. Usually these webpages are scraped repeatedly to observe changes and generate data streams.\n",
    "\n",
    "In a first step, we want to extract data from single webpages. To extract their data we can use the package `requests` because the communication is based on the HTTP protocol again. We will explain step by step how we can make requests to webpages - which is very similar to REST APIs. As examples we will use the webpages behind the news articles from the [BBC](https://www.bbc.com/) which we collected in the previous section. Then we will show you how to extract information from these webpages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b9584",
   "metadata": {},
   "source": [
    "### URLs\n",
    "\n",
    "Uniform resource locators (URLs) are references to all kinds of web resources. In the case of APIs, we called them endpoints. With webpages, we will not go into detail how they include parameters in their URLs. This is mainly due to the fact that many websites use parameters differently and not use a standard like REST APIs. Instead we will assume that our URLs are already given.\n",
    "\n",
    "In the following you can see the schema of a URL and two examples.\n",
    "\n",
    "```\n",
    "scheme:[//authority]path[?query][#fragment]\n",
    "```\n",
    "\n",
    "```\n",
    "https://www.bbc.com/news/world-middle-east-67084141\n",
    "https://en.wikipedia.org/wiki/Web_scraping#Techniques\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4aeb1d",
   "metadata": {},
   "source": [
    "### Requests\n",
    "\n",
    "There are several ways to send a request to a webpage, among others you can do it with the package `requests`. As mentioned in the previous session, this package allows you to make HTTP requests very easily and quickly. It provides all functions and methods to write your parameters into requests, send you requests and work with your responses.\n",
    "\n",
    "Let us first import the package or install it if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81786a08",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3136a230",
   "metadata": {},
   "source": [
    "In our example with the articles of the [BBC](https://www.bbc.com/) we are interested in getting the full texts of recent news articles. The best way to understand requests is to prepare an exemplary request on some news articles.\n",
    "\n",
    "Let us define one article's URL in the variable `url`. Then we pass this variables into the function `requests.get()` to make the corresponding request. This request is best wrapped inside a `try` statement, e.g. to handle an HTTP error `requests.exceptions.HTTPError`. An exception will also be raised by the method `raise_for_status()` when the response contains an invalid status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8fa03a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# define article\n",
    "url = \"https://www.bbc.com/news/world-middle-east-67084141\"\n",
    "\n",
    "try:\n",
    "    # make request\n",
    "    response = requests.get(url)\n",
    "    # check response\n",
    "    response.raise_for_status()\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(\"HTTPError: {}\".format(e))\n",
    "except Exception as e:\n",
    "    print(\"Error: {}\".format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20d0126",
   "metadata": {},
   "source": [
    "If the request works and throws no exception, you can have a first look into the response with the attribute `content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067f55a9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# inspect response\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5497fd9a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise</b>: Crawl the corresponding webpages of the news articles we collected in the previous session.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa78d15f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05acc7d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ebd012e",
   "metadata": {},
   "source": [
    "### Responses\n",
    "\n",
    "After a request has been sent to a webpage, you will receive a response to it. This response consists of different information, i.e. a status code, a header and a body. If you have made your request with the package `requests`, then you can easily access these information. With the attribute `content` on the response, we can access its body again, but this time it is not in JSON but HTML format - a markup with opening tags `<tag>` and closing tags `</tag>`.\n",
    "\n",
    "What follows is a very simple HTML document.\n",
    "\n",
    "```\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1> Title </h1>\n",
    "        <p> Full Text </p>\n",
    "    <body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa770494",
   "metadata": {},
   "source": [
    "In Python, you can parse HTML formatted strings with the use of the package `BeautifulSoup`. The package provides idiomatic ways of navigating, searching, and modifying HTML. In this way, we are aware of the structure und can extract certain information.\n",
    "\n",
    "Let us first import the package or install it if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7161cce8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45374ae",
   "metadata": {},
   "source": [
    "To convert an HTML string into a `BeautifulSoup` object, we have to pass the string and the corresponding parser `html.parser` into the class `BeautifulSoup`. On this object we will have various methods available to work with the HTML format. First, we will print out a structured form of the HTML with the method `prettify()`.\n",
    "\n",
    "Let us parse one webpage with `BeautifulSoup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fed91e5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# parse html\n",
    "soup = BeautifulSoup(webpages[2], \"html.parser\")\n",
    "\n",
    "# print structured html\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aaf4d1",
   "metadata": {},
   "source": [
    "Suppose you are now interested in a certain information in your HTML string. You can search for this information by its tag and attributes. For this, you can use the method `find()` which finds exactly one tag with the defined tag and attributes. However, if you want to find all tags that have the defined tag and attributes, then you better use the method `find_all()`. Afterwards, you can access the actual text behind these tags with the method `get_text()`.\n",
    "\n",
    "But before we can call these methods on our `BeautifulSoup` object, we first have to find out under which tags and attributes our information is hidden. The best way to do this is to open the corresponding webpage and search for the desired information. Once you have found it, you can right-click on it in your browser and select `Inspect` to see all the information about the underlying tag. If this method does not work for you, then you have to look into the HTML of the webpage and find the desired information by yourself. This can be done with `BeautifulSoup` together with `prettify()` or your own browser.\n",
    "\n",
    "Modern web browsers (Firefox, Chromium, IE, etc.) include a set of [web development tools](https://en.wikipedia.org/wiki/Web_development_tools). Originally addressed to web developers to test and debug the code (HTML, CSS, Javascript) used to build a web site, the browser web developer tools are the easiest way to explore and understand the technologies used to build a web site. The initial exploration later helps to scrape data from the web site.\n",
    "\n",
    "Let us try to access the full text of one article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a617c108",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# find tag with certain tag and id\n",
    "text = soup.find(\"main\", {\"id\": \"main-content\"})\n",
    "\n",
    "# print tag\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811b4e27",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# find all tags with certain tag and class\n",
    "paragraphs = soup.find_all(\"div\", {\"data-component\": \"text-block\"})\n",
    "\n",
    "# print number tags\n",
    "print(len(paragraphs))\n",
    "\n",
    "# append paragraphs to full text\n",
    "full_text = \" \".join(paragraph.get_text() for paragraph in paragraphs)\n",
    "\n",
    "# inspect full text\n",
    "print(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12e6091",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fe57632",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise</b>: Parse from your list of webpages the headlines and full texts.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5e822c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586dd32c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "369ece05",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise</b>: Save the headlines and full texts in a JSON file.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35630ef",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259fc115",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6eea8e66",
   "metadata": {},
   "source": [
    "## 4.2 Browser Automation\n",
    "\n",
    "Browser automation lets you execute actions automatically in a web browser for testing, web scraping or to perform repetitive tasks faster. These tasks include for example:\n",
    "\n",
    "- Load a page by URL including page dependencies (CSS, Javascript, images, media)\n",
    "- Simulate user interaction (clicks, input, scrolling)\n",
    "- Take screenshots\n",
    "- Access the DOM tree or the HTML modified by executed Javascript and user interactions from/in the browser to extract data\n",
    "\n",
    "There are two very popular libraries for browser automation:\n",
    "\n",
    "- [Selenium](https://pypi.org/project/selenium/)\n",
    "- [Playwright](https://playwright.dev/python/docs/intro)\n",
    "\n",
    "Note: Playwright does not run in a Jupyter notebook. We'll run the scripts directly in the Python interpreter.\n",
    "\n",
    "Installation:\n",
    "\n",
    "```\n",
    "playwright install\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-ceremony",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To test what Playright can do, we will create a script that will open a web page in two different browsers. Both times a web page is opened and a screenshot of the browser window is created.\n",
    "\n",
    "Let us have a look at this script.\n",
    "\n",
    "```python\n",
    "from playwright.sync_api import sync_playwright\n",
    "\n",
    "with sync_playwright() as p:\n",
    "    for browser_type in [p.chromium, p.firefox]:\n",
    "        browser = browser_type.launch()\n",
    "        page = browser.new_page()\n",
    "        page.goto('https://www.whatismybrowser.com/')\n",
    "        _ = page.screenshot(path=f'figures/example-{browser_type.name}.png')\n",
    "        browser.close()\n",
    "```\n",
    "\n",
    "Just run the script [scripts/playwright_whatsmyuseragent_screenshot.py](scripts/playwright_whatsmyuseragent_screenshot.py) in the console / shell:\n",
    "\n",
    "```\n",
    "python ./scripts/playwright_whatsmyuseragent_screenshot.py\n",
    "```\n",
    "\n",
    "The screenshots are then found in the folder `figures/` for [chromium](./figures/example-chromium.png) and [firefox](./figures/example-firefox.png)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-surge",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Playwright can record user interactions (mouse clicks, keyboard input) and create Python code to replay the recorded actions:\n",
    "\n",
    "```\n",
    "playwright codegen https://www.bundestag.de/abgeordnete/biografien\n",
    "```\n",
    "\n",
    "The created Python code is then modified, here to loop over all overlays showing the members of the parliament:\n",
    "\n",
    "```python\n",
    "from time import sleep\n",
    "\n",
    "from playwright.sync_api import sync_playwright\n",
    "\n",
    "def run(playwright):\n",
    "    browser = playwright.chromium.launch(headless=False)\n",
    "    context = browser.new_context(viewport={'height': 1080, 'width': 1920})\n",
    "    page = context.new_page()\n",
    "    page.goto(\"https://www.bundestag.de/abgeordnete/biografien\")\n",
    "    while True:\n",
    "        try:\n",
    "            sleep(3)\n",
    "            page.click(\"button:has-text(\\\"Vor\\\")\")\n",
    "        except Exception:\n",
    "            break\n",
    "\n",
    "with sync_playwright() as p:\n",
    "    run(p)\n",
    "```\n",
    "\n",
    "It's best to run the [replay script](./scripts/playwright_replay.py) in the console:\n",
    "\n",
    "```\n",
    "python ./scripts/playwright_replay.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8385473",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
