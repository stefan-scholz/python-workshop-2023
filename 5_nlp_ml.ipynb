{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "applicable-newsletter",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python Workshop\n",
    "# Session 5: Text Processing and Machine Learning\n",
    "\n",
    "Stefan Scholz\n",
    "\n",
    "In this fifth session we will get an overview of **natural language processing**. We will borrow techniques such as **linear regressions**, **classifications**, **pre-processing**, **tokenization**, **n-grams**, **vectorization** and **word embeddings**. These will enable us to train and evaluate a **text classifier**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50081cde",
   "metadata": {},
   "source": [
    "## 5.1 Natural Language Processing\n",
    "\n",
    "Natural language processing (NLP) is about programming computers to process and analyze natural language data (text and speech).\n",
    "\n",
    "For Python, there are two main NLP modules:\n",
    "- [spaCy](https://spacy.io/)\n",
    "- [NLTK](https://www.nltk.org/)\n",
    "\n",
    "Both modules implement the following NLP applications (and more), at least, for some languages:\n",
    "- Named entity recognition (NER)\n",
    "- Sentiment detection\n",
    "- Tokenization: splitting a text into words (aka tokens)\n",
    "- Part-of-speech tagging (POS)\n",
    "- Lemmatization: mapping a word in text to its base form (aka lemma)\n",
    "- Syntax parsing\n",
    "- Semantic representation of words\n",
    "\n",
    "We will first look into spaCy to explore NLP applications. Later, during the text classification we'll also touch some aspects of NLP, namely tokenization and the semantic representation of words in a vector space (word embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08915cab",
   "metadata": {},
   "source": [
    "### Text Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebf3452",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise</b>: Let us load as the text corpus the full texts we collected in the previous session. Store the full texts in a list called articles.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947c571e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c74e488",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e577b3e",
   "metadata": {},
   "source": [
    "### Find the Most Commonly Used Words in Articles\n",
    "\n",
    "We will now look into the articles itself and split the text into words, count word occurrences and generate a [word cloud](https://en.wikipedia.org/wiki/Tag_cloud) to visualize word frequencies or the \"importance\" of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4be96f1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1ed0dc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# define counter\n",
    "words = Counter()\n",
    "\n",
    "# loop over articles\n",
    "for article in articles:\n",
    "    # loop over words after splitting articles by spaces\n",
    "    for word in article.split():\n",
    "        word = word.lower()\n",
    "        words[word] += 1\n",
    "\n",
    "# print most common words\n",
    "print(words.most_common()[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaddbab4",
   "metadata": {},
   "source": [
    "This initial attempt shows that we need to skip over the most common functional words, in text processing called [stop words](https://en.wikipedia.org/wiki/Stop_word).\n",
    "\n",
    "Let us count word occurrences again but skip over stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c56c3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828cff8d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# get english stop words\n",
    "stop_words = set(get_stop_words('en'))\n",
    "\n",
    "def word_counts(articles):\n",
    "    words = Counter()\n",
    "    for article in articles:\n",
    "        for word in article.split():\n",
    "            word = word.lower()\n",
    "            if word in stop_words:\n",
    "                continue\n",
    "            words[word] += 1\n",
    "    return words\n",
    "\n",
    "# print most common words again\n",
    "print(word_counts(articles).most_common()[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49445ae3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5827cc1",
   "metadata": {},
   "source": [
    "### Word Clouds\n",
    "\n",
    "Word clouds are generated using the [wordcloud package](https://pypi.org/project/wordcloud/), see also:\n",
    "- [API docs of the WordCloud class](https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud.WordCloud)\n",
    "- more [examples](https://amueller.github.io/word_cloud/auto_examples/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d6c51c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa1bafa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# create word cloud\n",
    "wordcloud = WordCloud(width=400, height=400, background_color=\"lightgrey\").generate_from_frequencies(word_counts(articles))\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e4dc00",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50662944",
   "metadata": {},
   "source": [
    "### Tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer\n",
    "\n",
    "Let us look next into spaCy to explore NLP applications. Besides importing the spaCy library we have to also download the core modules for the processed language. These modules have to be downloaded only once.\n",
    "\n",
    " - [en_core_web_sm: English core module](https://spacy.io/models/en)\n",
    " - [de_core_news_sm: German core module](https://spacy.io/models/de)\n",
    "\n",
    "To download the module `en_core_web_sm` you can run the command `python -m spacy download en_core_web_sm`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e768e3e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e202251",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# load trained pipelines for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# apply pipeline to article\n",
    "doc = nlp(articles[3])\n",
    "\n",
    "# inspect resulting article\n",
    "doc.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-community",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-planet",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# filter tokens tagged as nouns\n",
    "list(filter(lambda t: t.pos_ == 'NOUN', doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-salon",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For some NLP applications, spaCy provides nice visualizations, for example, for named entities or syntax trees of dependency parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-prospect",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cbb795",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# display named entities\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-logging",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# display dependencies\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245e14b6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "alike-casting",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5.2 Machine Learning\n",
    "\n",
    "The field of machine learning is too broad to be fully introduced here. Please, see [Google's machine learning crash course](https://developers.google.com/machine-learning/crash-course/ml-intro). We'll focus on a couple of examples and introduce ML libraries written in or providing a Python API.\n",
    "\n",
    "- [scikit-learn](https://scikit-learn.org/): popular Python ML framework covering regression, classification and clustering using various approaches\n",
    "- [fastText](https://fasttext.cc/): a library for text classification and word representation learning with Python bindings\n",
    "- [TensorFlow](https://www.tensorflow.org/): ML framework with Python bindings focused on deep neural networks\n",
    "- [Keras](https://keras.io/): high-level API to Tensorflow\n",
    "- [PyTorch](https://pytorch.org/): competitor of Tensorflow\n",
    "- [Transformers](https://huggingface.co/transformers/): library to use, train and adapt [transformer deep learning models](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))\n",
    "\n",
    "Before we begin to look into Python ML examples, few ML key terms:\n",
    "- label: something we want to predict\n",
    "- feature: variable in the input (eg. numeric value, words)\n",
    "- example: data to learn from during training (labeled example) or to predict the label for using a learned model\n",
    "- model: a model is trained on labeled input data and later used to make predictions (\"infer\" labels) for unlabeled examples\n",
    "- regression vs. classification: labels are continuous vs. categorical values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-durham",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Regression and Classification with Scikit-Learn\n",
    "\n",
    "As an example for linear regression we take few trees from the tree cadastre used in [session 2](./2_structured_data.ipynb). We select a small subset of trees species to work with. We choose 3 trees quite different in shape: birch (tall and high, thinner trunk), lime tree (broad, thicker trunk) and apple tree (small, not tall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871e257c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-jesus",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# load trees as dataframe\n",
    "trees = pd.read_csv(\"data/KN_Baumkataster_2020.csv\")\n",
    "\n",
    "# rename columns in dataframe\n",
    "trees.rename(columns={\"hoeheM\": \"height (m)\", \"kronendurchmesserM\": \"treetop diameter (m)\", \"stammumfangCM\": \"trunk perimeter (cm)\"}, inplace=True)\n",
    "\n",
    "# define species\n",
    "species = [\"Betula pendula\", \"Tilia cordata\", \"Malus domestica\"]\n",
    "\n",
    "# define columns\n",
    "columns = [\"Name_lat\", \"trunk perimeter (cm)\", \"treetop diameter (m)\", \"height (m)\"]\n",
    "\n",
    "# select trees\n",
    "trees_selected = trees.loc[trees[\"Name_lat\"].isin(species), columns]\n",
    "\n",
    "# print selected trees\n",
    "trees_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3fadbb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# prepare a 3D plot to show how the trees are placed given the 3 metrics\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "\n",
    "# for loop over each species of trees\n",
    "for name, idx in trees_selected.groupby(\"Name_lat\").groups.items():\n",
    "    ax.scatter(*trees_selected.loc[idx, [\"trunk perimeter (cm)\", \"treetop diameter (m)\", \"height (m)\"]].T.values, label=name)\n",
    "\n",
    "ax.set_xlabel(\"trunk perimeter (cm)\")\n",
    "ax.set_ylabel(\"treetop diameter (m)\")\n",
    "ax.set_zlabel(\"height (m)\")\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1845344f",
   "metadata": {},
   "source": [
    "Let us predict the trunk perimeter and treetop diameter given the height using a linear regression.\n",
    "\n",
    "See also the scikit-learn documentation about [linear models](https://scikit-learn.org/stable/modules/linear_model.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50017fe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-authority",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# loop over species\n",
    "for sp in species:\n",
    "    # select rows by species\n",
    "    trees_sp = trees_selected.loc[trees_selected[\"Name_lat\"]==sp].dropna()\n",
    "\n",
    "    # convert metric cells to numpy arrays\n",
    "    height = trees_sp.loc[:, \"height (m)\"].values.reshape(-1,1)\n",
    "    treetop_trunk = trees_sp.loc[:, [\"trunk perimeter (cm)\", \"treetop diameter (m)\"]].values.reshape(-1,2)\n",
    "\n",
    "    # run linear regression\n",
    "    rgr = LinearRegression()\n",
    "    rgr.fit(height, treetop_trunk)\n",
    "\n",
    "    # print for certain species and height the trunk perimeter and treetop diameter\n",
    "    print(sp)\n",
    "    for height in [2, 5, 10, 15, 20]:\n",
    "        print(height, rgr.predict(np.array([[height]])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-count",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the following, we will use a [neural network classifier](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification) which uses as input features the 3 metric columns and tries to predict the species of a tree. How are the results given that only 3 tree species are used. What if we use more or even all species?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff525579",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-feeling",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# split data into train and test data (80% resp. 20% of the data)\n",
    "train, test = train_test_split(trees_selected.dropna(), test_size=0.2)\n",
    "\n",
    "# create multi-layer perceptron classifier\n",
    "cls = MLPClassifier(alpha=1, max_iter=1000)\n",
    "\n",
    "# prepare training data\n",
    "x_train = train[[\"trunk perimeter (cm)\", \"treetop diameter (m)\", \"height (m)\"]].values.reshape(-1,3)\n",
    "y_train = train[[\"Name_lat\"]].values.reshape(-1,1).ravel()\n",
    "\n",
    "# prepare testing data\n",
    "x_test = test[[\"trunk perimeter (cm)\", \"treetop diameter (m)\", \"height (m)\"]].values.reshape(-1,3)\n",
    "y_test = test[[\"Name_lat\"]].values.reshape(-1,1).ravel()\n",
    "\n",
    "# fit classifier\n",
    "cls.fit(x_train, y_train)\n",
    "\n",
    "# print results for predictions on test data\n",
    "y_predicted = cls.predict(x_test)\n",
    "print(f\"Classification report for classifier {cls}:\\n\", f\"{sklearn.metrics.classification_report(y_test, y_predicted)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a67312d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# print a confusion matrix: which tree species are predicted better? which ones are confused more often?\n",
    "cm = confusion_matrix(y_test, y_predicted, normalize=\"true\")\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=species)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773c817d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "animated-advance",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Text Classification with fastText\n",
    "\n",
    "[fastText](https://fasttext.cc/) is a software library for text classification and word representation learning. See the fastText tutorials for\n",
    "\n",
    "- [Text classification](https://fasttext.cc/docs/en/supervised-tutorial.html)\n",
    "- [Word representation learning](https://fasttext.cc/docs/en/unsupervised-tutorial.html)\n",
    "\n",
    "We will now follow the [fastText text classification](https://fasttext.cc/docs/en/supervised-tutorial.html) tutorial (cf. documentation of the [Python module \"fasttext\"](https://pypi.org/project/fasttext/)) to train and apply a text classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-activity",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will use the [Kaggle Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview) data set. In order to download the data set, you need to register at [Kaggle.com](https://www.kaggle.com/). Note: Kaggle is a good place to look and learn how other researchers and engineers tried to solve various ML problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b86952",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise</b>: Sign up for <a href=\"https://www.kaggle.com/\">Kaggle</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a45a0b4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53914c34",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise</b>: Download the <a href=\"https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data\">Toxic Comment Classification Challenge</a> and unpacked into the folder `data/kaggle-jigsaw-toxic`, you should see the tree files `train.csv`, `test.csv` and `test_labels.csv` in the mentioned folder.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e76b78",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd080fc9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dedbf7ec",
   "metadata": {},
   "source": [
    "Let us load the data and inspect the variables inside the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-program",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c099731",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# load training comments\n",
    "comments_train = pd.read_csv(\"data/kaggle-jigsaw-toxic/train.csv\")\n",
    "\n",
    "# select labels\n",
    "labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "# get means of each type of toxicity\n",
    "comments_train[labels].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583e4c0b",
   "metadata": {},
   "source": [
    "In the next step we have to tokenize the comments similar to what we have already done with the articles. And then we write it into a new file that fasttext can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c29f0b7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-christianity",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# initialize tokenizer\n",
    "tweet_tokenizer = TweetTokenizer(reduce_len=True)\n",
    "\n",
    "def tokenize(text):\n",
    "    global tweet_tokenizer\n",
    "    words = tweet_tokenizer.tokenize(text)\n",
    "    words = filter(lambda w: w != \"\" and w not in string.punctuation, words)\n",
    "    words = map(lambda w: w.lower(), words)\n",
    "    return ' '.join(words)\n",
    "\n",
    "tokenize(\"You're a hero! http://example.com/index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-combining",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# write data to fastText train file\n",
    "train_file = \"data/kaggle-jigsaw-toxic/train.txt\"\n",
    "\n",
    "def write_line_fasttext(fp, row):\n",
    "    global labels\n",
    "    line = ''\n",
    "    for label in labels:\n",
    "        if row[label] == 1:\n",
    "            if line:\n",
    "                line += ' '\n",
    "            line += '__label__' + label\n",
    "    if line:\n",
    "        line += ' '\n",
    "    else:\n",
    "        line += '__label__none '\n",
    "    line += tokenize(row['comment_text'])\n",
    "    fp.write(line)\n",
    "    fp.write('\\n')\n",
    "\n",
    "with open(train_file, 'w') as fp:\n",
    "    comments_train.apply(lambda row: write_line_fasttext(fp, row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e8f3a0",
   "metadata": {},
   "source": [
    "In the next step we can train our own text classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bb4e00",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-posting",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define train file\n",
    "train_file = \"data/kaggle-jigsaw-toxic/train.txt\"\n",
    "\n",
    "# create classifier with max length of word ngram 2 and minimal number of word occurences 2\n",
    "model = fasttext.train_supervised(input=train_file, wordNgrams=2, minCount=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-garbage",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# predict sample comment 1\n",
    "model.predict(tokenize(\"This is a well-written article.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17488122",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# predict sample comment 2\n",
    "model.predict(tokenize(\"Fuck you!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-knife",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# looking into the underlying word embeddings\n",
    "model.get_nearest_neighbors(\"idiot\", k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-forward",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_file = \"data/kaggle-jigsaw-toxic/model.bin\"\n",
    "model.save_model(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a6b5d3",
   "metadata": {},
   "source": [
    "Now that we have trained our models, let us evalate it on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-mumbai",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# read test files as data frames\n",
    "comments_test = pd.read_csv(\"data/kaggle-jigsaw-toxic/test.csv\")\n",
    "comments_test_labels = pd.read_csv(\"data/kaggle-jigsaw-toxic/test_labels.csv\")\n",
    "\n",
    "# join both tables\n",
    "comments_test = comments_test.merge(comments_test_labels, on=\"id\")\n",
    "\n",
    "# skip rows not labelled / not used\n",
    "comments_test  = comments_test.loc[comments_test['toxic'] != -1]\n",
    "\n",
    "# write test set for fastText\n",
    "test_file = \"data/kaggle-jigsaw-toxic/test.txt\"\n",
    "with open(test_file, 'w') as fp:\n",
    "    comments_test.apply(lambda row: write_line_fasttext(fp, row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-carpet",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# test model on test file (returns support, precision, recall)\n",
    "model.test(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14f6a09",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# test model pe each label\n",
    "res_per_label = model.test_label(test_file)\n",
    "\n",
    "for label in res_per_label.items():\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-worst",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "developmental-bankruptcy",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transformer Language Models and the Transformers Library\n",
    "\n",
    "[Transformer language models](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) are used to address a couple of NLP tasks -- text classification, text generation, translation and more. [Hugging Face's transformers library](https://huggingface.co/transformers/) provides an powerful and easy to learn interface to use transformers. Hugging Face also offers a large repository of transformer models shared by a growing community of researchers and organizations. For more details exceeding the examples below, see the [transformers course](https://huggingface.co/course).\n",
    "  \n",
    "Transformers can be \"fine-tuned\" to a specific task, see [training of transformers](https://huggingface.co/transformers/training.html). Adding a task-specific head to a transformer pre-trained on large amounts of training data (usually 100 GBs or even TBs of text) saves resources spent for training and can overcome the problem of not enough training data. Manually labelling training data is expensive and naturally puts a limit on the amount of training data. But even if the vocabulary in the training data is limited, there's a good chance that the pre-trained transformer has seen the unknown words in the huge data used for pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fc94dc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-essex",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "p = pipeline(\"fill-mask\", model=\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-initial",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# print sequences (with filled mask)\n",
    "for s in p(\"He works as a [MASK] in a clinic.\"):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-kinase",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# print sequences (with filled mask)\n",
    "for s in p(\"He works as a [MASK] at the Zeppelin university.\"):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6c0cf6",
   "metadata": {},
   "source": [
    "To see which other tasks can be done by this pipeline just call the function `help` on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-instrument",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "help(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a501eb6",
   "metadata": {},
   "source": [
    "Let us use a pipeline to make a sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-essay",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# create a pipeline for sentiment analysis\n",
    "p = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# get the sentiment for a sample sentence\n",
    "p(\"I'm happy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-cooling",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# get the sentiment for a sample sentence 2\n",
    "p(\"I'm sad.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0722332",
   "metadata": {},
   "source": [
    "Let us use a pipeline to translate a text from German to English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-birthday",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# create pipeline for translation\n",
    "p = pipeline(\"translation\", model=\"facebook/wmt19-de-en\")\n",
    "\n",
    "# get the translation\n",
    "p(\"\"\"Nicht nur unterschiedliche Berechnungen bereiten Kopfzerbrechen.\n",
    "  Bei der Eigenwahrnehmung zeigt sich: In Deutschland gibt es massive\n",
    "  Missverständnisse über Ausmaß und Art von Ungleichheit.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043bf1e8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
